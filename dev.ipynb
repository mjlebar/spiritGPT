{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e428af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ad122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1255699\n"
     ]
    }
   ],
   "source": [
    "f = open('phg.txt', 'r', encoding='UTF-8')\n",
    "text = f.read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd2cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE: ON SCIENTIFIC \n",
      "COGNITION \n",
      "\n",
      "\n",
      "1. It is customary to preface a work with an explanation of \n",
      "the author’s aim, why he wrote the book, and the relationship \n",
      "in which he believes it to stand to other earlier or contemporary \n",
      "treatises on the same subject. In the case of a philosophical \n",
      "work, however, such an explanation seems not only superfluous \n",
      "but, in view of the nature of the subject-matter, even inappro¬ \n",
      "priate and misleading. For whatever might appropriately be \n",
      "said about philosophy in a preface—say a historical statement \n",
      "of the main drift and the point of view, the general content and \n",
      "results, a string of random assertions and assurances about \n",
      "truth—none of this can be accepted as the way in which to \n",
      "expound philosophical truth. Also, since philosophy moves \n",
      "essentially in the element of universality, which includes within \n",
      "itself the particular, it might seem that here more than in any \n",
      "of the other sciences the subject-matter itself, and even in its \n",
      "complete nature\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1498c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[79, 25,  1, 75, 74,  1, 80, 68],\n",
      "        [75, 74,  1, 69, 74,  1, 80, 68],\n",
      "        [72,  1, 76, 78, 69, 74, 63, 69],\n",
      "        [34, 49, 37, 38, 32, 30, 41,  1]]), tensor([[25,  1, 75, 74,  1, 80, 68, 65],\n",
      "        [74,  1, 69, 74,  1, 80, 68, 65],\n",
      "        [ 1, 76, 78, 69, 74, 63, 69, 76],\n",
      "        [49, 37, 38, 32, 30, 41,  1, 44]]))\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 #length of context before character prediction\n",
    "batch_size = 4 #number of examples simultaneously processed\n",
    "\n",
    "#make the vocabulary\n",
    "vocab = sorted(list(set(text)))\n",
    "\n",
    "#make the encoding and decoding (text to number, number to text)\n",
    "encoder = {ch:i for i, ch in enumerate(vocab)}\n",
    "decoder = {i:ch for i, ch in enumerate(vocab)}\n",
    "def encode_string(string): \n",
    "    return [encoder[char] for char in string]\n",
    "def decode_string(string):\n",
    "    return ''.join([decoder[char] for char in string])\n",
    "\n",
    "#split into training and validation sets\n",
    "data = torch.tensor(encode_string(text), dtype=torch.long)\n",
    "portion_train = int(0.9*len(data))\n",
    "training_set=data[:portion_train]\n",
    "val_set = data[portion_train:]\n",
    "\n",
    "\n",
    "#make up the batches (including padding + start & end tokens)\n",
    "def get_batch(type):\n",
    "    data = training_set if type == 'train' else val_set\n",
    "    indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    inputs = torch.stack([data[index: index+block_size] for index in indices], dim=0)\n",
    "    targets = torch.stack([data[index+1: index+block_size+1] for index in indices], dim=0)\n",
    "    return inputs, targets\n",
    "\n",
    "print(get_batch('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9590aab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.functional' has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/9qbm5d512bxd8q8nsjwjv7rm0000gn/T/ipykernel_17713/1112432534.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m3/9qbm5d512bxd8q8nsjwjv7rm0000gn/T/ipykernel_17713/1112432534.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msa_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msa_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#note to self - why do we do softmax here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msa_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#self-attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         out = sa_weights @ v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.functional' has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(32, head_size, bias=False)\n",
    "        self.query = nn.Linear(32, head_size, bias=False)\n",
    "        self.value = nn.Linear(32, head_size, bias=False)\n",
    "        self.register_buffer('triangle', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, block, channels = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        sa_weights = k @ q.transpose(1, 2) * channels ** -0.5 \n",
    "        sa_weights = sa_weights.masked_fill(self.triangle[:block, :block]==0, float('-inf'))\n",
    "        #note to self - why do we do softmax here?\n",
    "        sa_weights = F.softmax(weights, dim = 2)\n",
    "        #self-attention weights\n",
    "#         out = sa_weights @ v\n",
    "        return sa_weights\n",
    "    \n",
    "x = torch.randn(4, 8, 32)\n",
    "head = Head(16)\n",
    "print(head(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133385b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
